{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b335764-33a1-4a7d-b592-859f792f20fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing MAIZE ===\n",
      "MAIZE - Valid data rows: 6139284\n",
      "--- Climate correlations ---\n",
      "ENSO vs Yield Anomaly: 0.0300\n",
      "Temp vs Yield Anomaly: 0.0445\n",
      "Drought vs Yield Anomaly: -0.0287\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, when, concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"YieldClimateCorrelation\").getOrCreate()\n",
    "\n",
    "# User sets the crop here (one at a time)\n",
    "target_crop = \"maize\"  # Change to: \"maize\", \"wheat\", \"soybean\" as needed\n",
    "\n",
    "# Define paths\n",
    "data_folder = \"csv_data\"\n",
    "climate_folder = \"climate_data\"\n",
    "crop_file = os.path.join(data_folder, f\"{target_crop}.csv\")\n",
    "\n",
    "enso_file = os.path.join(climate_folder, \"ENSO.csv\")\n",
    "temperature_file = os.path.join(climate_folder, \"temperature.csv\")\n",
    "drought_file = os.path.join(climate_folder, \"drought.csv\")\n",
    "plot_folder = \"plots\"\n",
    "os.makedirs(plot_folder, exist_ok=True)\n",
    "\n",
    "# Load climate datasets\n",
    "enso_df = spark.read.csv(enso_file, header=True, inferSchema=True) \\\n",
    "    .select(col(\"Year\"), col(\"`Nino 3.4 SST Anomalies`\").alias(\"ENSO_Index\")) \\\n",
    "    .filter((col(\"Year\") >= 1982) & (col(\"Year\") <= 2016)) \\\n",
    "    .dropna()\n",
    "\n",
    "temp_df = spark.read.csv(temperature_file, header=True, inferSchema=True) \\\n",
    "    .groupBy(\"Year\").agg(mean(\"Temp_Anomaly\").alias(\"Temp_Anomaly\"))\n",
    "\n",
    "drought_df = spark.read.csv(drought_file, header=True, inferSchema=True) \\\n",
    "    .groupBy(\"Year\").agg(mean(\"Drought_Index\").alias(\"Drought_Index\"))\n",
    "\n",
    "# Function to compute 5-year moving average and yield anomalies\n",
    "def compute_anomalies(df):\n",
    "    windowSpec = Window.partitionBy(\"Grid_ID\").orderBy(\"Year\").rowsBetween(-4, 0)\n",
    "    df = df.withColumn(\"MovingAvg\", mean(\"Yield\").over(windowSpec))\n",
    "    df = df.withColumn(\"Anomaly\", col(\"Yield\") - col(\"MovingAvg\"))\n",
    "    df = df.withColumn(\"StdDev\", stddev(\"Anomaly\").over(windowSpec))\n",
    "    df = df.withColumn(\"ExtremeLow\", when(col(\"Anomaly\") < -2 * col(\"StdDev\"), 1).otherwise(0))\n",
    "    df = df.withColumn(\"ExtremeHigh\", when(col(\"Anomaly\") > 2 * col(\"StdDev\"), 1).otherwise(0))\n",
    "    return df\n",
    "\n",
    "# Load and process target crop\n",
    "print(f\"\\n=== Processing {target_crop.upper()} ===\")\n",
    "df = spark.read.csv(crop_file, header=True, inferSchema=True)\n",
    "df = df.filter((col(\"year\") >= 1982) & (col(\"year\") <= 2016)) \\\n",
    "       .withColumn(\"Grid_ID\", concat_ws(\"_\", col(\"lat\").cast(\"string\"), col(\"lon\").cast(\"string\"))) \\\n",
    "       .select(col(\"year\").alias(\"Year\"), \"Grid_ID\", col(\"var\").alias(\"Yield\"))\n",
    "\n",
    "df_anomalies = compute_anomalies(df)\n",
    "\n",
    "# Join with global-averaged climate data\n",
    "df_joined = df_anomalies.join(enso_df, on=\"Year\", how=\"left\") \\\n",
    "                         .join(temp_df, on=\"Year\", how=\"left\") \\\n",
    "                         .join(drought_df, on=\"Year\", how=\"left\")\n",
    "\n",
    "df_clean = df_joined.dropna(subset=[\"Anomaly\", \"ENSO_Index\", \"Temp_Anomaly\", \"Drought_Index\"])\n",
    "print(f\"{target_crop.upper()} - Valid data rows: {df_clean.count()}\")\n",
    "\n",
    "# Compute and display correlations\n",
    "correlations = {\n",
    "    \"ENSO\": df_clean.stat.corr(\"Anomaly\", \"ENSO_Index\"),\n",
    "    \"Temp\": df_clean.stat.corr(\"Anomaly\", \"Temp_Anomaly\"),\n",
    "    \"Drought\": df_clean.stat.corr(\"Anomaly\", \"Drought_Index\"),\n",
    "}\n",
    "print(\"--- Climate correlations ---\")\n",
    "for name, val in correlations.items():\n",
    "    print(f\"{name} vs Yield Anomaly: {val:.4f}\")\n",
    "\n",
    "# Plot and save\n",
    "df_plot = df_clean.select(\"Year\", \"Anomaly\", \"ENSO_Index\").orderBy(\"Year\").toPandas()\n",
    "if not df_plot.empty:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(df_plot[\"Year\"], df_plot[\"Anomaly\"], label=f\"{target_crop.capitalize()} Yield Anomaly\")\n",
    "    plt.plot(df_plot[\"Year\"], df_plot[\"ENSO_Index\"], label=\"ENSO Index\", linestyle=\"--\")\n",
    "    plt.title(f\"{target_crop.capitalize()} Yield Anomaly vs ENSO Index\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Anomaly / ENSO Index\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(plot_folder, f\"{target_crop}_enso_plot.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\" Plot saved: {plot_path}\")\n",
    "else:\n",
    "    print(f\" No data to plot for {target_crop.upper()}\")\n",
    "\n",
    "# Free memory\n",
    "del df, df_anomalies, df_joined, df_clean, df_plot\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e367209-8289-4b9a-9762-ff31e76cea83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
